{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has two objects: pd.Series object and pd.DataFrame object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "3    4\n",
       "4    5\n",
       "5    6\n",
       "6    7\n",
       "7    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create series array\n",
    "record = pd.Series([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see only the values of the series\n",
    "record.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=8, step=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see only the index of the series\n",
    "record.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get each element of series by index\n",
    "record[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3\n",
       "3    4\n",
       "4    5\n",
       "5    6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also perform slice on a series object\n",
    "record[2:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    6\n",
       "6    7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slicing from the series using the negative index\n",
    "record[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a    10\n",
       "b    11\n",
       "c    12\n",
       "d    13\n",
       "e    14\n",
       "f    15\n",
       "g    16\n",
       "h    17\n",
       "i    18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specifying your index \n",
    "record2 = pd.Series([10, 11, 12, 13, 14, 15, 16, 17, 18], index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'])\n",
    "record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting items from the series using the specified index\n",
    "record2['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "c    12\n",
       "d    13\n",
       "e    14\n",
       "f    15\n",
       "g    16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selection from the series using slice indexing\n",
    "record2['c':'g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    20\n",
       "3    21\n",
       "6    22\n",
       "1    23\n",
       "8    24\n",
       "4    25\n",
       "2    26\n",
       "7    27\n",
       "0    28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using noncontiguous or nonsequential indeces\n",
    "record3 = pd.Series([20,21,22,23,24,25,26,27,28], index=[5,3,6,1,8,4,2,7,0])\n",
    "record3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accessing series item by its index\n",
    "record3[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different ways to create series object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Enugu     3400000\n",
       "Imo       2300000\n",
       "Kaduna    3600000\n",
       "Kano      4600000\n",
       "Lagos     6400000\n",
       "Oyo       2500000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from a specialized dictionary\n",
    "states = {'Oyo':2500000, 'Lagos':6400000, 'Kano':4600000, 'Kaduna':3600000, 'Enugu':3400000, 'Imo':2300000}\n",
    "statesdata = pd.Series(states)\n",
    "statesdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data can be indexed using the state name\n",
    "statesdata['Lagos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imo       2300000\n",
       "Kaduna    3600000\n",
       "Kano      4600000\n",
       "Lagos     6400000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also perform slicing on the data\n",
    "statesdata['Imo':'Lagos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Series objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10\n",
       "2    10\n",
       "3    10\n",
       "4    10\n",
       "5    10\n",
       "6    10\n",
       "7    10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# series can be created through a scalar data\n",
    "values = pd.Series(10, index=[1,2,3,4,5,6,7])\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O   NaN\n",
       "L   NaN\n",
       "A   NaN\n",
       "E   NaN\n",
       "R   NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index can be explicitly set if different result is prefered\n",
    "valuekey = pd.Series({1:'Oyo', 2:'Lagos', 3:'Abuja', 4:'Enugu', 5:'Rivers'}, \n",
    "                     index=['O', 'L', 'A','E','R'])\n",
    "valuekey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Enugu     4000\n",
       "Imo       1500\n",
       "Kaduna    3000\n",
       "Kano      7000\n",
       "Lagos     2000\n",
       "Oyo       5000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmass = pd.Series({'Oyo':5000, 'Lagos':2000, 'Kano':7000, 'Kaduna':3000, 'Enugu':4000, \n",
    "                      'Imo':1500})\n",
    "landmass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Enugu</th>\n",
       "      <td>3400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Imo</th>\n",
       "      <td>2300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kaduna</th>\n",
       "      <td>3600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kano</th>\n",
       "      <td>4600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lagos</th>\n",
       "      <td>6400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oyo</th>\n",
       "      <td>2500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Population\n",
       "Enugu      3400000\n",
       "Imo        2300000\n",
       "Kaduna     3600000\n",
       "Kano       4600000\n",
       "Lagos      6400000\n",
       "Oyo        2500000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe from single dictionary\n",
    "census = pd.DataFrame({'Population':statesdata})\n",
    "census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Land mass</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Enugu</th>\n",
       "      <td>4000</td>\n",
       "      <td>3400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Imo</th>\n",
       "      <td>1500</td>\n",
       "      <td>2300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kaduna</th>\n",
       "      <td>3000</td>\n",
       "      <td>3600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kano</th>\n",
       "      <td>7000</td>\n",
       "      <td>4600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lagos</th>\n",
       "      <td>2000</td>\n",
       "      <td>6400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oyo</th>\n",
       "      <td>5000</td>\n",
       "      <td>2500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Land mass  Population\n",
       "Enugu        4000     3400000\n",
       "Imo          1500     2300000\n",
       "Kaduna       3000     3600000\n",
       "Kano         7000     4600000\n",
       "Lagos        2000     6400000\n",
       "Oyo          5000     2500000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe from multiple dictionaries\n",
    "pop_census = pd.DataFrame({'Population':statesdata, 'Land mass':landmass})\n",
    "pop_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Land mass</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Benue</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edo</th>\n",
       "      <td>7000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enugu</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Imo</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>2300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kaduna</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kano</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lagos</th>\n",
       "      <td>2000.0</td>\n",
       "      <td>6400000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oyo</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2500000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rivers</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Land mass  Population\n",
       "Benue      4000.0         NaN\n",
       "Edo        7000.0         NaN\n",
       "Enugu         NaN   3400000.0\n",
       "Imo        1500.0   2300000.0\n",
       "Kaduna     3000.0   3600000.0\n",
       "Kano          NaN   4600000.0\n",
       "Lagos      2000.0   6400000.0\n",
       "Oyo           NaN   2500000.0\n",
       "Rivers     5000.0         NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating dataframe with some missing values\n",
    "land = pd.Series({'Rivers':5000, 'Lagos':2000, 'Edo':7000, 'Kaduna':3000, 'Benue':4000, 'Imo':1500})\n",
    "census = pd.DataFrame({'Population':statesdata, 'Land mass':land})\n",
    "census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Enugu', 'Imo', 'Kaduna', 'Kano', 'Lagos', 'Oyo'], dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list out the index \n",
    "pop_census.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Land mass', 'Population'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list out the columns on the table\n",
    "pop_census.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   4000, 3400000],\n",
       "       [   1500, 2300000],\n",
       "       [   3000, 3600000],\n",
       "       [   7000, 4600000],\n",
       "       [   2000, 6400000],\n",
       "       [   5000, 2500000]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list out all the values of the dataframe\n",
    "pop_census.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Enugu     4000\n",
       "Imo       1500\n",
       "Kaduna    3000\n",
       "Kano      7000\n",
       "Lagos     2000\n",
       "Oyo       5000\n",
       "Name: Land mass, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indexing a dataframe through it column name\n",
    "pop_census['Land mass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the landmass of Lagos using the column name and index name\n",
    "pop_census['Land mass']['Lagos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Land mass</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kaduna</th>\n",
       "      <td>3000</td>\n",
       "      <td>3600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kano</th>\n",
       "      <td>7000</td>\n",
       "      <td>4600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lagos</th>\n",
       "      <td>2000</td>\n",
       "      <td>6400000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Land mass  Population\n",
       "Kaduna       3000     3600000\n",
       "Kano         7000     4600000\n",
       "Lagos        2000     6400000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slicing the dataframe using indexing\n",
    "pop_census['Kaduna':'Lagos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Land mass</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kaduna</th>\n",
       "      <td>3000</td>\n",
       "      <td>3600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Land mass  Population\n",
       "Kaduna       3000     3600000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give us the complete details of Kaduna state\n",
    "pop_census['Kaduna':'Kaduna']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.keys of         Land mass  Population\n",
       "Enugu        4000     3400000\n",
       "Imo          1500     2300000\n",
       "Kaduna       3000     3600000\n",
       "Kano         7000     4600000\n",
       "Lagos        2000     6400000\n",
       "Oyo          5000     2500000>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_census.keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing DataFrame Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Sunday\n",
       "1       Sayo\n",
       "2     Charse\n",
       "3       Rose\n",
       "4    Micheal\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe from a single series object\n",
    "student = pd.Series(['Sunday', 'Sayo', 'Charse', 'Rose', 'Micheal'])\n",
    "student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sayo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Micheal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Student Name\n",
       "0       Sunday\n",
       "1         Sayo\n",
       "2       Charse\n",
       "3         Rose\n",
       "4      Micheal"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the series to dataframe specifying the column name and setting new index\n",
    "studentrec= pd.DataFrame(student, columns=['Student Name'],index=[0,1,2,3,4])\n",
    "studentrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Male</th>\n",
       "      <th>Female</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Js1</th>\n",
       "      <td>0.516866</td>\n",
       "      <td>0.072279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Js2</th>\n",
       "      <td>0.859326</td>\n",
       "      <td>0.373104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Js3</th>\n",
       "      <td>0.228204</td>\n",
       "      <td>0.980915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Male    Female\n",
       "Js1  0.516866  0.072279\n",
       "Js2  0.859326  0.373104\n",
       "Js3  0.228204  0.980915"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe from multidimension numpy array\n",
    "studentpop = pd.DataFrame(np.random.rand(3,2), columns=['Male', \"Female\"], index=['Js1', 'Js2', 'Js3'])\n",
    "studentpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Male</th>\n",
       "      <th>Female</th>\n",
       "      <th>Total Student</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Js1</th>\n",
       "      <td>0.516866</td>\n",
       "      <td>0.072279</td>\n",
       "      <td>0.589145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Js2</th>\n",
       "      <td>0.859326</td>\n",
       "      <td>0.373104</td>\n",
       "      <td>1.232429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Js3</th>\n",
       "      <td>0.228204</td>\n",
       "      <td>0.980915</td>\n",
       "      <td>1.209119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Male    Female  Total Student\n",
       "Js1  0.516866  0.072279       0.589145\n",
       "Js2  0.859326  0.373104       1.232429\n",
       "Js3  0.228204  0.980915       1.209119"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding a new column to already existing dataframe object\n",
    "studentpop['Total Student'] = studentpop['Female'] + studentpop['Male']\n",
    "studentpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51686626, 0.07227861, 0.58914487],\n",
       "       [0.85932576, 0.37310374, 1.23242949],\n",
       "       [0.22820372, 0.98091483, 1.20911855]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the values of the studentpop\n",
    "studentpop.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51686626, 0.07227861, 0.58914487])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns the values on the first row\n",
    "studentpop.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Js1</th>\n",
       "      <th>Js2</th>\n",
       "      <th>Js3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Male</th>\n",
       "      <td>0.516866</td>\n",
       "      <td>0.859326</td>\n",
       "      <td>0.228204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female</th>\n",
       "      <td>0.072279</td>\n",
       "      <td>0.373104</td>\n",
       "      <td>0.980915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Student</th>\n",
       "      <td>0.589145</td>\n",
       "      <td>1.232429</td>\n",
       "      <td>1.209119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Js1       Js2       Js3\n",
       "Male           0.516866  0.859326  0.228204\n",
       "Female         0.072279  0.373104  0.980915\n",
       "Total Student  0.589145  1.232429  1.209119"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can inverse between index and column, T means transpose\n",
    "studentpop.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Index object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of index in numpy array\n",
    "list1 = np.array([1,2,3,4,5,6,7,8,9])\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 7, 9])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using fancy indexing\n",
    "fancy = [0,3,6,8]\n",
    "list1[fancy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2, 3, 5, 7, 11], dtype='int64')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating index object in pandas\n",
    "ind = pd.Index([2, 3, 5, 7, 11])\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index as immutable array\n",
    "ind[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([2, 5, 11], dtype='int64')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index object can also be sliced\n",
    "ind[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 (5,) 1 int64\n"
     ]
    }
   ],
   "source": [
    "# index object also have the same numpy attributs\n",
    "print(ind.size, ind.shape, ind.ndim, ind.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Index does not support mutable operations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-3c4bb357bc55>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# index object as immutable  array cannot be modified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mind\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1723\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1724\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Index does not support mutable operations\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Index does not support mutable operations"
     ]
    }
   ],
   "source": [
    "# index object as immutable  array cannot be modified\n",
    "ind[2] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index can be used to perform set operations\n",
    "indA = pd.Index([1, 3, 5, 7, 9])\n",
    "indB = pd.Index([2, 3, 5, 7, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intersection\n",
    "indA & indB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# union\n",
    "indA | indB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetric difference\n",
    "indA ^ indB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Indexing and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data selection in Series\n",
    "# Series as dictionary\n",
    "data = pd.Series([0.25, 0.5, 0.75, 1.0], index=['a', 'b', 'c', 'd'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting using index\n",
    "data['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use dictionary-like python expression and methods to check for key/indices and values\n",
    "#check for index\n",
    "'a' in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for key\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for values\n",
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for both key/indices and values\n",
    "list(data.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series object can be modified just like dictionary using dictionary-like sytax\n",
    "# adding new value to series\n",
    "data['e'] = 1.25\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series as one-demensional array\n",
    "# slicing by explicit index\n",
    "data['a':'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing by explicit integer index\n",
    "data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking \n",
    "data[(data>0.3) & (data<0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fancy indexig\n",
    "data[['a', 'c', 'e']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing with loc, iloc and ix\n",
    "data.loc['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Selection in DataFrame\n",
    "# DataFrame as dictionary\n",
    "pop_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select based on column names\n",
    "pop_census['Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also selection using the column name as an attribute-style \n",
    "pop_census.Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare if the two methods are equal\n",
    "pop_census.Population is pop_census['Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can modify the values of dataframe like adding new columns\n",
    "pop_census['Density']=pop_census['Population'] / pop_census['Land mass']\n",
    "pop_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the values of the DataFrame as two dimensional arry\n",
    "pop_census.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the index of the DataFrame as a vector\n",
    "pop_census.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can transpose the full dataframe to swap rows and columns\n",
    "pop_census.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing a dataframe object\n",
    "pop_census.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing using the column name\n",
    "pop_census['Land mass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using loc, iloc and ix\n",
    "# use iloc with explicit integer index\n",
    "pop_census.iloc[:3, :2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use loc for explicit index name\n",
    "pop_census.loc['Enugu':'Imo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ix as an hybrid of loc and iloc\n",
    "# pop_census.ix[:3, :'Population'] this is already deprecated in the new version of pandas \n",
    "# so use this method \n",
    "pop_census.iloc[:3]['Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentpop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc can be combine with masking and fancy indexing \n",
    "studentpop.loc[studentpop['Total Student'] > 0.356, ['Male','Female']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_census.loc[pop_census.Density > 1533.333333, ['Land mass','Population']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any of this method can be used to modify the values\n",
    "pop_census.iloc[0, 2] = 50000\n",
    "pop_census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing using the rows\n",
    "pop_census['Lagos':'Enugu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing using row integer index\n",
    "pop_census[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct masking can also be done row-wise instead of column-wise\n",
    "pop_census[pop_census.Density > 1200.000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ufuncs in Pandas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# unive create a series object\n",
    "rng = np.random.RandomState(42)\n",
    "ser = pd.Series(rng.randint(0, 10, 4))\n",
    "ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a dataframe object\n",
    "df = pd.DataFrame(rng.randint(0, 10, (3, 4)), columns=['A', 'B', 'C', 'D'])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform an exponential ufunc on the series \n",
    "np.exp(ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also perform more complex operation by combining multiple ufunc together\n",
    "np.sin(df * np.pi/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can apply the same ufunc on our pop_census data\n",
    "np.sin(pop_census * np.pi/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us try to practice with a real dataset using california city data\n",
    "data = pd.read_csv('C:\\\\Users\\\\OWNER\\\\Datasets\\\\california_cities.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us describe the data to see general overview of the whole dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try and verify any of the details described above\n",
    "data['latd'].mean(), data['latd'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us get some other informations about the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the shape of the data. other details includes (ndim, dtype, size) \n",
    "data.shape, data.ndim, data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the values of the data inform of np multidimension array\n",
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the columns of the data inform of np array\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the index of the array \n",
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can modify the data by adding new column \"Density\"\n",
    "data['Density']= data['population_total'] / data['area_total_sq_mi']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing a column name from the data\n",
    "change_column = np.array(data.columns)\n",
    "change_column[0] = 'S/N'\n",
    "change_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now lets try and receate the datafame with the new column\n",
    "data = pd.DataFrame(data.values, columns = change_column)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us drop all columns without any data in them\n",
    "data.drop(['Unnamed: 14', 'Unnamed: 15'], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names are also an attribute of pd object so can be used to refference the column i.e\n",
    "data.city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.population_total.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ufunc allignment\n",
    " allignment is how pandas merges two datasets and manages any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how allignment work with series object\n",
    "ser1 = pd.Series([2,4,6], index=[0, 1, 2])\n",
    "ser2 = pd.Series([1, 3, 5], index=[1, 2, 3])\n",
    "print(ser1)\n",
    "print(ser2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any operation performed on NaN will return NaN\n",
    "ser1 + ser2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raplace any NaN with zero\n",
    "ser1.add(ser2, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how allignment work with dataframe object\n",
    "rng =np.random.RandomState()\n",
    "val1 = pd.DataFrame(rng.randint(0,20,(2,2)), columns=list('AB'))\n",
    "val2 = pd.DataFrame(rng.randint(0,10, (3,3)), columns=list('BAC'))\n",
    "print(val1)\n",
    "print(val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let try to add the two datasets together\n",
    "val1 + val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first row of val2 and add it to val2\n",
    "val2 + val2.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean of val1 and use it to fill any missing value\n",
    "fill = val1.stack().mean()\n",
    "fill\n",
    "val1.add(val2, fill_value=fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ufunc operations between Dataframe and Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng =np.random.RandomState()\n",
    "valA = rng.randint(10, size=(3, 4))\n",
    "valA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract one row of valA from itself\n",
    "valA - valA[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame from the data\n",
    "df_val = pd.DataFrame(valA, columns=list('QRST'))\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract one row of valA from itself\n",
    "df_val - df_val.\n",
    "iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use object method to operate on the dataframe column-wise\n",
    "df_val.subtract(df_val['R'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice df_val at index of row=0  and column= from begining to end at step of 2 ********\n",
    "halfrow = df_val.iloc[0,::2]\n",
    "halfrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val - halfrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python uses None to represent missing data\n",
    "x = np.array([1, 2, None, 3, 5, 6])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas uses NaN to represent missing data\n",
    "y = np.array([1, 2, np.nan, 3, 5, 6])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Any arithmetic operation with a non type will raise error\n",
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any arithmetic operation with NaN will results in NaN output\n",
    "y.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 *np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y), np.min(y), np.max(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using np.nan with the operation will overlook NaN and compute only the values available\n",
    "np.nansum(y), np.nanmin(y), np.nanmax(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN and None Pandas\n",
    "# pandas will converts any none type to NaN\n",
    "pd.Series([1, np.nan, 2, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(range(2), dtype=int)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0] = None\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operating on Null Values\n",
    "pandas has two useful methods for detecting null data: isnull() and notNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.Series([1, np.nan, 'Tunde', None])\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isNull() will return true for those values that contains null data\n",
    "dft.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notNull() will return true for those fields that contains value\n",
    "dft.notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get the value of those that are null \n",
    "dft[dft.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also get the values of those that contains data\n",
    "a = dft.notnull()\n",
    "dft[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this same operation on our data above\n",
    "data[data.isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.notnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting Null Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with series object\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will remove all null and nan value and left us only with a clean data\n",
    "dft.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with a dataframe object\n",
    "dfdata = pd.DataFrame([[1, np.nan, 2, np.nan ], [2, 3, 5, np.nan],[np.nan, 4, 6, np.nan]],\n",
    "                      columns=list('ABCD'))\n",
    "dfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default dropna will drop all rows in which any null is present\n",
    "dfdata.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can specify to drop along columns\n",
    "dfdata.dropna(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will can specify how to drop nan along row or column. how='any' is the default if no specified\n",
    "dfdata.dropna(axis='columns', how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how='all' mean drop any column where all values are nan\n",
    "dfdata.dropna(axis='columns', how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh let us specify a minimum number of non-null value for row or column to drop\n",
    "dfdata.dropna(axis='columns', thresh=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any rows that has less than three non-null\n",
    "dfdata.dropna(axis='rows', thresh=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying dropna to our data\n",
    "data.dropna(axis='columns').isnull().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis='columns', how='all').isnull().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling null value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill a null value in series object\n",
    "srdata = pd.Series([1, np.nan, 2, np.nan ], index=list(\"abcd\"))\n",
    "srdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill every null value with a default zero value\n",
    "srdata.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also fill any null value with the mean of the data\n",
    "srdata.fillna(srdata.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ffill mean forward fill i.e use the previous value to fill the next null\n",
    "srdata.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bfill mean backward fill i.e use the subsquent value the fill the previous null\n",
    "srdata.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill a null value in dataframe object\n",
    "dfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fil forward row-wise\n",
    "dfdata.fillna(method='ffill', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill forward column-wise\n",
    "dfdata.fillna(method='ffill', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill every null values with default value zero\n",
    "dfdata.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the null values with mean\n",
    "dfdata.fillna(dfdata.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column the fill with dafualt nan\n",
    "dfdata['E']=(np.nan)\n",
    "dfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can then be filled with zeros\n",
    "dfdata.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill some specified columns with default value \n",
    "dfdata[['E']] = dfdata[['E']].fillna(5)\n",
    "dfdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill some specified columns with zero \n",
    "data[['area_total_sq_mi','area_water_sq_mi']] = data[['area_total_sq_mi',\n",
    "                                                      'area_water_sq_mi']].fillna(0)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment\n",
    "\n",
    "# import the car_price dataset\n",
    "# extract all the values that is not null from the dataset to a new variable\n",
    "# extract all the null values from the dataset into a new variable\n",
    "# drop all columns containing null value from the original dataset\n",
    "# clean the data set so it doesnt have any value again\n",
    "# compute the sum, mean of the price column on the list\n",
    "# get the car with lowest price and with highest price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = pd.read_csv('C:\\\\Users\\\\OWNER\\\\Datasets\\\\housing\\\\housing.csv')\n",
    "house.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house['latitude'].mean(), house['latitude'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house['longitude'].mean(), house['longitude'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.shape, house.ndim, house.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this same operation on our data above\n",
    "housenan = house[house.total_bedrooms.isnull()]\n",
    "housenan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housenan = house[house.total_bedrooms.notnull()]\n",
    "housenan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying dropna to our data\n",
    "house.dropna(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[housenan.population.mean(), housenan.population.std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[housenan.population.sum(), housenan.population.sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " housenan.households[housenan.population.values.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housenan.households[housenan.population.values.argmin()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housenan.households[1017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housenan.households.values.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housenan.households.values.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housenan.households[16013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housenan.households[9782]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Indexed Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiindex can be created using dictionary of tuple as key , THE KEY IS TURPLE AND THE VALUE IS SCAKLAR \n",
    "data = {('California', 2000): 33871648, ('California', 2010): 37253956, ('Texas', 2000): 20851820, \n",
    "        ('Texas', 2010): 25145561, ('New York', 2000): 18976457, ('New York', 2010): 19378102}\n",
    "pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiindex can also be created using a list of tuples \n",
    "index = [('California', 2000), ('California', 2010), ('New York', 2000), ('New York', 2010), ('Texas',\n",
    "            2000), ('Texas', 2010)]  \n",
    "populations = [33871648, 37253956, 18976457, 19378102, 20851820, 25145561]       \n",
    "pop = pd.Series(populations, index=index)       \n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice pop and give us population beteen Colifornia 2010 and Texas 2000\n",
    "pop[('California', 2010) : ('Texas', 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give us population value where year is 2010\n",
    "pop[[i for i in pop.index if i[1]==2010]]\n",
    "# for i in pop.index:\n",
    "#     if i[1]==2010:\n",
    "#         print(pop[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an index can be created from tuple using a list of tuples\n",
    "index= pd.MultiIndex.from_tuples(index)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to add the index to the data by using reindex function\n",
    "pop = pop.reindex(index)\n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with reindex it is now possible to slice by year\n",
    "pop[:, 2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack is used to turn one of the index to column name and convert the series to dataframe\n",
    "popdf = pop.unstack()\n",
    "popdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack will turn the dataframe back to its series form \n",
    "popdf.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Index DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = pd.DataFrame({'Total': pop, 'Under18': [9267089, 9284094, 4687374, 4318033, 5906301, 6879014]})        \n",
    "pop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_tuples(index)       \n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pop_df.reindex(index)       \n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under18 = pop_df['Under18']/pop_df['Total']\n",
    "under18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under18.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using unstack function will turn the year as sub-column \n",
    "pop_df = pop.unstack()       \n",
    "pop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_df = pop_df.stack()       \n",
    "pop_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.rand(4, 2), \n",
    "index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]], columns=['data1', 'data2'])        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.rand(9, 2), \n",
    "                  index=[['Oyo', 'Oyo', 'Oyo', 'Rivers','Rivers','Rivers','Lagos','Lagos','Lagos'],\n",
    "                         [2000,2001,2002,2000,2001,2002,2000,2001,2002]], columns=['data1', 'data2'])        \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MultiIndex using Constructors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat multiIndex using from_tuples constructor\n",
    "cm1 = pd.DataFrame(np.random.rand(4, 2), pd.MultiIndex.from_tuples([(\"Class 1\", 'a'), (\"Class 1\", 'b'), (\"Class 2\", 'a'),\n",
    "                                                                    (\"Class 2\", 'b')]), columns=[\"Male\", \"Female\"])\n",
    "cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "df = pd.DataFrame(np.random.rand(9, 2), pd.MultiIndex.from_tuples([('Oyo',2000),('Oyo',2001),\n",
    "                    ('Oyo',2002),('Rivers',2000),('Rivers',2001),('Rivers',2002),('Lagos',2000),\n",
    "                    ('Lagos',2001),('Lagos',2002)]), columns=['data1', 'data2'])        \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat multiIndex using from_arrays constructor\n",
    "cm = pd.DataFrame(np.random.rand(4, 2), pd.MultiIndex.from_arrays([['a', 'a', 'b', 'b'], [1, 2, 1, 2]]))        \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "cm = pd.DataFrame(np.random.rand(9, 2), pd.MultiIndex.from_arrays([['Oyo', 'Oyo', 'Oyo', 'Rivers',\n",
    "                    'Rivers','Rivers','Lagos','Lagos','Lagos'], [2000,2001,2002,2000,2001,2002,2000,\n",
    "                    2001,2002]]))        \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat multiIndex using from_product constructor\n",
    "cm = pd.DataFrame(np.random.rand(4, 2),pd.MultiIndex.from_product([['a', 'b'], [1, 2]]))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "cm = pd.DataFrame(np.random.rand(9, 2),pd.MultiIndex.from_product([['Oyo', 'Rivers', 'Lagos'], [2000, \n",
    "                                                                    2001,2002]]))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also construct multiIndex directly using internal encoding by levels and labels(codes)\n",
    "cm = pd.MultiIndex(levels=[['a', 'b', 'c'], [1, 2, 3]], codes=[[0, 0, 1, 1,2,2], [0, 1,2, 0, 1,2]])\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the index to dataframe\n",
    "ex = pd.DataFrame(populations, pd.MultiIndex.from_tuples(cm))\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = pd.DataFrame(populations, pd.MultiIndex.from_arrays(\n",
    "                [['California', 'California', 'New York', 'New York', 'Texas', 'Texas'], \n",
    "                [2000, 2010, 2000, 2010, 2000, 2010]]), columns=['Population'])\n",
    "ex1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multiIndex manually\n",
    "ex = pd.DataFrame(populations,pd.MultiIndex(levels=[['California', 'New York', 'Texas'], \n",
    "                                [2000, 2010]], codes=[[0, 0, 1, 1, 2, 2], [0, 1, 0, 1, 0, 1]]))\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = pd.DataFrame(populations, pd.MultiIndex.from_product([['California', 'New York', 'Texas'],\n",
    "                                                           [2000,2010]]), columns=['population'])\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving a column name to your index\n",
    "ex1.index.names = ['state', 'year']        \n",
    "ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiIndex for Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical indices and columns \n",
    "index = pd.MultiIndex.from_product([[2013, 2014], [1, 2]],  names=['year', 'visit']) \n",
    "\n",
    "columns = pd.MultiIndex.from_product([['Bob', 'Guido', 'Sue'], ['HR', 'Temp']], \n",
    "                                     names=['patient', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock some data \n",
    "data = np.round(np.random.randn(4, 6), 1)\n",
    "data[:, ::2] *= 10\n",
    "data += 37\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the DataFrame \n",
    "health_data = pd.DataFrame(data, index=index, columns=columns) \n",
    "health_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get all data of a particular patient\n",
    "health_data['Guido']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the Temp data of a particular patient\n",
    "health_data[\"Guido\"][\"Temp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 2013 data of a particular patient \n",
    "health_data.iloc[0][\"Guido\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return 2013 Temp data of a particular patient\n",
    "health_data.iloc[0][\"Guido\"][\"Temp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try this with a school record data and create a multiIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data from csv\n",
    "school_data = pd.read_csv(\"C:\\\\Users\\\\OWNER\\\\Datasets\\\\student record.csv\")\n",
    "school_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical and columns  \n",
    "columns = pd.MultiIndex.from_product([['English','Maths', 'Social Study', 'Agriculture'],\n",
    "                                      ['SS1','SS2','SS3']], names=['subject', 'Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the name as series object\n",
    "# student_name= school_data.reindex(school_data['Name'])\n",
    "# student_name\n",
    "school_data = school_data.set_index(\"Name\")\n",
    "school_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_name = school_data.index\n",
    "student_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_data.columns=columns\n",
    "school_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the values as numpy array\n",
    "student_score =school_data.values\n",
    "student_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hierarchical indices and generate the serial number\n",
    "sn = np.arange(1,11)\n",
    "index = pd.MultiIndex.from_arrays([sn, student_name], names=['S/N', 'Student Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the new dataframe with multiIndex value\n",
    "report_card = pd.DataFrame(student_score, index=index, columns=columns)\n",
    "report_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data using an index \n",
    "report_card[6:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract English score for ss1 class for all students\n",
    "report_card['English', 'SS1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first six row and first six column of the data\n",
    "report_card.iloc[:6, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the English score for ss3 using loc function\n",
    "report_card.loc[:, (\"English\", \"SS3\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of indexSlice and use it to specify the slicing\n",
    "idx = pd.IndexSlice\n",
    "report_card.loc[idx[:,\"Success Edom\"], idx[:,\"SS3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can perform aggregates on multiIndex e.g sum(), max(), mean(), etc.\n",
    "mean_value = report_card.sum(axis=1,level=\"Class\")\n",
    "mean_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Datasets: Concat and Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exmple with numpy array\n",
    "x = [1,2,3,4]\n",
    "y = [5,6,7,8]\n",
    "z = [9,10,11,13]\n",
    "np.concatenate([x,y,z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with multidimension np array\n",
    "a = [[1,2,3,4],[5,6,7,8]]\n",
    "b = [[11,12,13,14],[15,16,17,18]]\n",
    "np.concatenate([a,b], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how it works with pd.concat series\n",
    "ser1 = pd.Series(['a','b','c','d'], index=[1,2,3,4])\n",
    "ser2 = pd.Series(['e','f','g','i'], index=[1,6,2,8])\n",
    "print('ser1 = \\n', ser1, \"\\n\")\n",
    "print('ser2 = \\n', ser2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two series objects\n",
    "pd.concat([ser1,ser2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how it works with pd.concat dataframe \n",
    "# pd.concat allows for duplicate of index unlike np array\n",
    "df1 = pd.DataFrame([['a','b'],['c','d']], index=[1,2], columns=['One', 'Two'])\n",
    "df2 = pd.DataFrame([['a','b'],['c','d']], index=[1,2], columns=['One', 'Two'])\n",
    "print('df1 = \\n', df1)\n",
    "print('df2 = \\n', df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's concatenate the two dataframe objects\n",
    "pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the concatenation can also be done along axis but row is the default\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the axis can also be specified by name\n",
    "pd.concat([df1,df2], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but to stop duplicate of index use verify_integrity parameter set to True\n",
    "pd.concat([ser1,ser2], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ignore the index and let pd.concate create its own index set \n",
    "# ignore_index to True\n",
    "pd.concat([ser1,ser2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with duplicate indices, setting verify_integrity=True will raise error but adding \n",
    "# ignore_index=True can make it to work \n",
    "ser3 = pd.Series(['a','b','c','d'], index=[1,2,3,4])\n",
    "ser4 = pd.Series(['e','f','g','i'], index=[2,6,3,8])\n",
    "pd.concat([ser1,ser2], verify_integrity=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also use the multiIndex key to specify label to the data\n",
    "pd.concat([ser1,ser2], keys=['A','B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating with joins\n",
    "df1 = pd.DataFrame([['a','b','c'],['d','e','f']], index=[1,2], columns=['A', 'B', 'C'])\n",
    "df2 = pd.DataFrame([['b','c','d'],['g','h','i']], index=[1,2], columns=['B', 'C', 'D'])\n",
    "print('df1 = \\n', df1)\n",
    "print('df2 = \\n', df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer join is merging of all exiting columns on both dataframe objects. this is the default merge \n",
    "pd.concat([df1, df2], join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner merging will only merger based on column common to both dataframe objects\n",
    "pd.concat([df1,df2], join='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join_axes will join based on df2 columns available\n",
    "# pd.concat([df1, df2], join_axes=[df2.columns]) join_axes is deprecated in this version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use the append function of pd to join two series or dataframe\n",
    "ser1.append(ser2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append function also works on dataframe object\n",
    "df1.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combining datasets: Merge and Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee':['Bob', 'Jake', 'Lisa', 'Charse'], \n",
    "                    'Department':['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'employee':['Bob', 'Jake', 'Lisa', 'Charse'], \n",
    "                    'Hire_date':[2004, 2008, 2012, 2014]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-to-one join\n",
    "one2one = pd.merge(df1, df2)\n",
    "one2one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many-to-one\n",
    "df3 = pd.DataFrame({'Department':['Accounting', 'Engineering', 'HR'], \n",
    "                    'Supervisor':['Carly', 'Rosemary', 'Stephen'] })\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many2one= pd.merge(one2one, df3)\n",
    "many2one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many-to-many\n",
    "df4 = pd.DataFrame({'Department':['Accounting', 'Accounting', 'Engineering', 'Engineering', \n",
    "                    'HR', 'HR'], 'Skills':['Agriculture', 'Physics', 'Engligh', 'Chemistry',\n",
    "                    'Computer', 'Mechanical']})\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merge =pd.merge(many2one, df4)\n",
    "new_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# specification of the merge key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the 'on' keyword\n",
    "pd.merge(df1, df2, on='employee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merge =pd.merge(many2one, df4, on=\"Department\")\n",
    "new_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the 'left_on' amd right_on keyword \n",
    "df5 = pd.DataFrame({'Name':['Bob', 'Jake', 'Lisa', 'Charse'], \n",
    "                    'Salary':[70000, 80000, 120000, 90000]})\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df5, left_on='employee', right_on='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to drop 'name'column so as to remove redundant column\n",
    "pd.merge(df1, df5, left_on='employee', right_on='Name').drop('Name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merging using index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first turn the employee column of each table to index and using the left_index and\n",
    "# right_index\n",
    "df1ind = df1.set_index('employee')\n",
    "df2ind = df2.set_index(\"employee\")\n",
    "print(df1ind); print(df2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1ind, df2ind, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the join function which use index key by default\n",
    "df1ind.join(df2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using both index and column name to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using left_index with right_on \n",
    "pd.merge(df1ind, df5, left_index=True, right_on='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using left_on with righ_index\n",
    "pd.merge(df5, df1ind, right_index=True, left_on='Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merging using set arithmetic for joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.DataFrame({'Name':['Peter', 'Paul', 'Mary'], 'Food':['Fish', 'Beans','Bread']}, \n",
    "                   columns=['Name', 'Food'])\n",
    "\n",
    "df7 = pd.DataFrame({'Name':['Mary', 'Joseph'], 'Drink':['Wine', 'Beer']}, \n",
    "                   columns=['Name', 'Drink'])\n",
    "print(df6,\"\\n\"); print(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the inner join by default\n",
    "pd.merge(df6, df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the inner join\n",
    "pd.merge(df6, df7, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the outer join\n",
    "pd.merge(df6, df7, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the inner left\n",
    "pd.merge(df6, df7, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the right join\n",
    "pd.merge(df6, df7, how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with overlaping column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na1 = pd.DataFrame({'Name':['Sunday', 'Joel', 'Johnson', 'Charse'], \n",
    "                    'position':['CEO', 'MD', 'Supervisor', 'Manager']})\n",
    "\n",
    "na2 = pd.DataFrame({'Name':['Sunday', 'Joel', 'Johnson', 'Charse'], \n",
    "                    'position':['Supervisor', 'CEO', 'Manager', 'MD']})\n",
    "print(na1,'\\n'); print(na2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid overlapping column names the merger will append a surfix automatically to make \n",
    "# each column unique\n",
    "pd.merge(na1, na2, on='Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# you can also specify the surfix to use \n",
    "pd.merge(na1, na2, on='Name', suffixes=['_College', '_Ibadan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Working with US State Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_pop = pd.read_csv(\"C:\\\\Users\\\\OWNER\\\\Datasets\\\\us_state_population.csv\") \n",
    "us_area = pd.read_csv(\"C:\\\\Users\\\\OWNER\\\\Datasets\\\\us_state_area.csv\") \n",
    "us_abbrevs = pd.read_csv(\"C:\\\\Users\\\\OWNER\\\\Datasets\\\\us_states_abbriev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_area.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_abbrevs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let merge the abbreviation table and population table on the outer join\n",
    "abb_pop = pd.merge(us_pop, us_abbrevs, how=\"outer\", left_on=\"state/region\", right_on=\"abbreviation\")\n",
    "abb_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have two columns conveying the same information and we need to drop one\n",
    "abb_pop = abb_pop.drop(labels=[\"abbreviation\"], axis=1)\n",
    "abb_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's double-caheck if there were any mismatches in the data using nulls:\n",
    "abb_pop.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us see which of the datas are null\n",
    "abb_pop[abb_pop[\"population\"].isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let figure out which region has is mismatch data\n",
    "abb_pop.loc[abb_pop[\"state\"].isnull(), \"state/region\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows that PR has no state and USA has no abbreviation so let's fix this error\n",
    "abb_pop.loc[abb_pop[\"state/region\"]==\"PR\", \"state\"]=\"Puerto Rico\"\n",
    "abb_pop.loc[abb_pop[\"state/region\"]==\"USA\", \"state\"]=\"United States\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check again if there is still any column with null value\n",
    "abb_pop.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us see the final table after error correction\n",
    "abb_pop.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can now merge the result with US area data using a similar columns\n",
    "final_merge = pd.merge(abb_pop, us_area, on=\"state\", how=\"left\")\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check if there are any mismatched data in the final_merge table\n",
    "final_merge.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lets  check for the region with null data\n",
    "final_merge[\"state\"][final_merge[\"area (sq. mi)\"].isnull()].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us drop the null values because the population density of the entire United State is \n",
    "# not relevant to our current discussion\n",
    "final_merge.dropna(inplace=True)\n",
    "final_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check if there are any mismatched data in the final_merge table\n",
    "final_merge.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets select the portion of the data corresponding with the year 2010 and the total population.\n",
    "data2010 =final_merge.query(\"year == 2010 & ages == 'total'\")\n",
    "data2010.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets compute the population density and display it in order. we will start by reindenxing\n",
    "# our data on the state and then compute the result\n",
    "data2010.set_index('state', inplace=True)\n",
    "density = data2010['population']/data2010['area (sq. mi)']\n",
    "density.sort_values(ascending=True, inplace=True)\n",
    "density.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us get the state with the highest landmax\n",
    "final_merge['state'][final_merge['area (sq. mi)'].values.argmax()]\n",
    "# state = us_area['state]\n",
    "# landmax = us_area['area(sq. mi)]\n",
    "# state[landmax]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popind = final_merge['area (sq. mi)'].values.argmax()\n",
    "popind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge['population'][popind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge['population'][final_merge['area (sq. mi)'].values.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge['population'][final_merge['area (sq. mi)'].values.argmin()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple aggregation in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing aggregation on Series object\n",
    "rng = np.random.RandomState(42)\n",
    "agser = pd.Series(rng.rand(5))\n",
    "agser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sum = '+str(agser.sum()))\n",
    "print('Minimum = ',agser.min())\n",
    "print('Median = ',agser.median())\n",
    "print('Maximum = ',agser.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean = ',agser.mean())\n",
    "print('Standart Dev = ',agser.std())\n",
    "print('Variance = ',agser.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a DataFrame, by default the aggregates return results within each column:\n",
    "df = pd.DataFrame({'A': rng.rand(5),'B': rng.rand(5)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default the aggragation will be done on column based\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By specifying the axis argument, you can instead aggregate within each row:\n",
    "df.mean(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a convenience method describe() that computes several common aggregates for\n",
    "# each column and returns the result.\n",
    "planets.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Aggregation Description\n",
    "#count() Total number of items\n",
    "#first(), last() First and last item\n",
    "#mean(), median() Mean and median\n",
    "#min(), max() Minimum and maximum\n",
    "#std(), var() Standard deviation and variance\n",
    "#mad() Mean absolute deviation\n",
    "#prod() Product of all items\n",
    "#sum() Sum of all items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing aggregation on DataFrame object\n",
    "report_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_card.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sum along the row\n",
    "report_card.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sum along columns and give the max score\n",
    "print(report_card.sum(axis='columns'), \"\\n\")\n",
    "print('Highest score = ',report_card.sum(axis='columns').max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the student with the max score\n",
    "# std_rec = report_card.sum(axis='columns'); \n",
    "# score = report_card.sum(axis='columns').max()\n",
    "# std_rec[std_rec >= score]\n",
    "report_card[report_card.sum(axis='columns')>= report_card.sum(axis='columns').max()].sum(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the student with the max score in English\n",
    "english = report_card['English'].sum(axis='columns')[report_card['English'].sum(\n",
    "                        axis='columns')>=report_card['English'].sum(axis='columns').max()]\n",
    "english\n",
    "# breakdown of the above one line code\n",
    "# english = report_card['English'].sum(axis='columns')\n",
    "# max =english.max()\n",
    "# english[english>=max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract a student with his english record only\n",
    "report_card.loc[3]['Charse Akpan':'Charse Akpan']['English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first three student with his english record only\n",
    "report_card.iloc[:3]['English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment\n",
    "# import califonia cities\n",
    "# describe the data set\n",
    "# convert the city column to be index\n",
    "# city that has the highest population\n",
    "# calculate the sum along column axis\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GroupBy: Spliting, Apply and Combine\n",
    "#Spliting: split the table into their various group\n",
    "#Apply: apply aggregation on the data\n",
    "#Combine: combine the group back into a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame({\"key\":['A','B','C','A','B','C'], \"data\":range(6)})\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can compute the most basic split-apply-combine operation with the groupby() method of \n",
    "# DataFrame by passong the name of the desired key column\n",
    "data_df.groupby('key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the groupby returns an object of the groupby. But will returns an output of the table only when\n",
    "# we perform an aggregation on the groupby object.\n",
    "data_df.groupby('key').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Planets data as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will use the Planets dataset, available via the Seaborn package. online conection is \n",
    "# requred to load this data\n",
    "import seaborn as sns\n",
    "planets = sns.load_dataset('planets')\n",
    "planets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's practise this with the planets data above.\n",
    "planets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the groupby object support column indexing in the same way as the dataframe and returns a \n",
    "# modified groupby object\n",
    "planets.groupby(\"method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the groupby can also be done on specific column on the dataframe object\n",
    "planets.groupby(\"method\")['orbital_period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As with the GroupBy object, no computation is done until we call some aggregate on the object:\n",
    "planets.groupby(\"method\").median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate can also be done on specific column on the dataframe object\n",
    "planets.groupby(\"method\")['orbital_period'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use the describe() method of DataFrames to perform a set of aggregations that describe\n",
    "# each group in the data:\n",
    "planets.groupby(\"method\")['year'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us practice the groupby with new_merger data above\n",
    "new_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us groupby the enployee column\n",
    "new_merge.groupby('employee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using describe function to see the output of the groupby\n",
    "new_merge.groupby('employee')['Skills'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate, filter, transform, apply\n",
    "GroupBy objects have aggregate(), filter(), transform(), and apply() methods that efficiently implement a variety of\n",
    "useful operations before combining the grouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of the following subsections, we’ll use this DataFrame:\n",
    "rng= np.random.RandomState(0)\n",
    "df_gen = pd.DataFrame({\"key\":['A','B','C','A','B','C'], \"data\":range(6), 'data2':rng.randint(0, 10, 6)})\n",
    "df_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate() can take a string, a function, or a list thereof, and compute all the aggregates \n",
    "# at once. \n",
    "df_gen.groupby('key').aggregate(['min', np.median, max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another useful pattern is to pass a dictionary mapping column names to operations\n",
    "# to be applied on that column:\n",
    "df_gen.groupby('key').aggregate({'data':'min','data2':max})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A filter() function allows you to drop data based on the group properties. For example, we \n",
    "# might want to keep all groups in which the standard deviation is larger than some critical value\n",
    "def filter_func(x):\n",
    "    return x['data2'].std()>4\n",
    "print(df_gen.groupby('key').std());\n",
    "df_gen.groupby('key').filter(filter_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform() return some transformed version of the full data to recombine.\n",
    "# A common example is to center the data by subtracting the group-wise mean:\n",
    "df_gen.groupby('key').transform(lambda x: x - x.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The apply() method lets you apply an arbitrary function to the group results. The function should \n",
    "# take a DataFrame, and return either a Pandas object (e.g., DataFrame, Series) or a scalar; \n",
    "def norma_data(x):\n",
    "    x['data']/=x['data2'].sum()\n",
    "    return x\n",
    "df_gen.groupby('key').apply(norma_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the split key: A list, array, series, or index providing the grouping keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The key can be any series or list with a length matching that of the DataFrame. For example:\n",
    "L = [0,1,0,1,0,1]\n",
    "print(df_gen)\n",
    "df_gen.groupby(L).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more verbose way of accomplishing the df.groupby('key') from before:\n",
    "print(df_gen)\n",
    "df_gen.groupby('key').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method is to provide a dictionary that maps index values to the group keys:\n",
    "keyind =df_gen.set_index('key')\n",
    "print(keyind)\n",
    "mapping = {'A':'vowel', 'B':'consonant', 'C':'consonant'}\n",
    "keyind.groupby(mapping).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'A':'vowel', 'B':'consonant', 'C':'ungroup'}\n",
    "keyind.groupby(mapping).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to mapping, you can pass any Python function that will input the index value and \n",
    "# output the group:\n",
    "print(keyind); \n",
    "keyind.groupby(str.lower).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping example: Using planets dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  in a couple lines of Python code we can put all these together and count discovered planets \n",
    "#     by method and by decade:\n",
    "decade = 10 * (planets['year'] // 10)\n",
    "decade = decade.astype(str) + 's'\n",
    "decade.name = 'decade'\n",
    "planets.groupby(['method', decade])['number'].sum().unstack().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we’ll use the database of passengers on the Titanic, available through the Seaborn library\n",
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start learning more about this data, we might begin by grouping it according to\n",
    "# gender, survival status, or some combination thereof. Let’s look at survival rate by gender:\n",
    "titanic.groupby('sex')[['survived']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the vocabulary of GroupBy, we might proceed using something like this: we group by class \n",
    "# and gender, select survival, apply a sum aggregate, combine the resulting groups, and then \n",
    "# unstack the hierarchical index\n",
    "titanic.groupby(['sex', 'class'])['survived'].aggregate('sum').unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the equivalent to the preceding operation using the pivot_table method of DataFrames:\n",
    "titanic.pivot_table('survived', index='sex', columns='class', aggfunc={'survived':sum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilevel pivot tables\n",
    "Just as in the GroupBy, the grouping in pivot tables can be specified with multiple levels, and via a number of options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might be interested in looking at age as a third dimension. We’ll bin the age using the pd.cut \n",
    "age = pd.cut(titanic['age'], [0, 18, 80])\n",
    "titanic.pivot_table('survived', ['sex', age], 'class', aggfunc={'survived':sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can apply this same strategy when working with the columns as well; let’s add info on the\n",
    "# fare paid using pd.qcut to automatically compute quantiles:\n",
    "fare = pd.qcut(titanic['fare'], 2)\n",
    "titanic.pivot_table('survived', ['sex', age], [fare, 'class'], aggfunc={'survived':sum},  fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The full call signature of the pivot_table method of DataFrames is as follows:\n",
    "#DataFrame.pivot_table(data, values=None, index=None, columns=None,\n",
    "aggfunc='mean', fill_value=None, margins=False,\n",
    "dropna=True, margins_name='All')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aggfunc keyword controls what type of aggregation is applied, which is a mean by default. \n",
    "# Additionally, it can be specified as a dictionary mapping a column to any of the above desired\n",
    "# options:\n",
    "titanic.pivot_table(index='sex', columns='class', aggfunc={'survived':sum, 'fare':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it’s useful to compute totals along each grouping. This can be done via the margins keyword:\n",
    "titanic.pivot_table('survived', index='sex', columns='class', margins=True, aggfunc={\n",
    "                        'survived':sum}, margins_name=\"Total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Using US Birthrate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = pd.read_csv('C:\\\\PythonClass\\\\Data Science\\\\Datasets\\\\us_births.csv')\n",
    "births.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check for brief details around the data \n",
    "births.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get more discription on the dataset\n",
    "births.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s add a decade column, so we can group base on decade\n",
    "births['decade'] = 10 * (births['year'] // 10)\n",
    "births.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can start to understand this data a bit more by using a pivot table. Lets take a look \n",
    "# at male and female births as a function of decade:\n",
    "births.pivot_table('births', index='decade', columns='gender', aggfunc='sum', margins=True,\n",
    "                   margins_name=\"Total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see this trend a bit more clearly, we can use the built-in plotting tools in Pandas to\n",
    "# visualize the total number of births by year\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set() # use Seaborn styles\n",
    "births.pivot_table('births', index='year', columns='gender', aggfunc='sum').plot()\n",
    "plt.ylabel('total births per year');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further data exploration\n",
    "Though this doesn’t necessarily relate to the pivot table, there are a few more interesting features we can pull out of this dataset using the Pandas tools covered up to this\n",
    "point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must start by cleaning the data a bit, removing outliers caused by mistyped dates (e.g\n",
    "# June 31st) or missing values (e.g., June 99th).\n",
    "quartiles = np.percentile(births['births'], [25, 50, 75])\n",
    "mu = quartiles[1]\n",
    "sig = 0.74 * (quartiles[2] - quartiles[0])\n",
    "sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 0.74 comes from the interquartile range of a Gaussian distribution. \n",
    "# With this we can use the query() method to filter out rows with births outside these values:\n",
    "births = births.query('(births > @mu - 5 * @sig) & (births < @mu + 5 * @sig)')\n",
    "print(births.describe(), \"\\n\")\n",
    "births.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 'day' column to integer; it originally was a string due to nulls\n",
    "births['day'] = births['day'].astype(int)\n",
    "print(births.info(),\"\\n\")\n",
    "births.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can combine the day, month, and year to create a Date index. This allows us to \n",
    "# quickly compute the weekday corresponding to each row:\n",
    "births.index = pd.to_datetime(births.year.astype(str) + births.month.astype(str) + \n",
    "                              births.day.astype(str), format='%Y%m%d')\n",
    "births.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quickly compute the weekday corresponding to each row:\n",
    "births['dayofweek'] = births.index.dayofweek\n",
    "births.index.name = \"Date of birth\"\n",
    "births.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this we can plot births by weekday for several decade:\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "births.pivot_table('births', index='dayofweek', columns='decade', aggfunc='mean').plot()\n",
    "plt.gca().set_xticklabels(['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun'])\n",
    "plt.ylabel('mean births by day');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another interesting view is to plot the mean number of births by the day of the year.\n",
    "# Let’s first group the data by month and day separately:\n",
    "births_by_date = births.pivot_table('births', [births.index.month, births.index.day])\n",
    "births_by_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make this easily plottable, let’s turn these months and days into a date by associating them\n",
    "# with a dummy year variable (making sure to choose a leap year so February 29th is correctly handled!)\n",
    "births_by_date.index = [pd.datetime(2012, month, day) for (month, day) in births_by_date.index]\n",
    "births_by_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focusing on the month and day only, we now have a time series reflecting the average number of \n",
    "# births by date of the year. From this, we can use the plot method to plot the data \n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "births_by_date.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For arrays of strings, NumPy does not provide such simple access, and thus you’re stuck using \n",
    "# a more verbose loop syntax:\n",
    "data = ['peter', 'paul', 'mary', 'rose', 'sunday', 'rechard', 'charse']\n",
    "[s.capitalize() for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is perhaps sufficient to work with some data, but it will break and raise an error if \n",
    "# there are any missing values. For example:\n",
    "data = ['peter', 'Paul', None, 'MARY', 'charse']\n",
    "[s.capitalize() for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas includes features to address both this need for vectorized string operations and for \n",
    "# correctly handling missing data via the str attribute of Pandas Series and Index objects \n",
    "# containing strings.\n",
    "name = ['peter', 'paul', 'mary', 'rose', None, 'sunday', 'rechard', 'charse']\n",
    "name_pd = pd.Series(name)\n",
    "name_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now call a single method that will capitalize all the entries, while skipping\n",
    "# over any missing values:\n",
    "name_pd.str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using tab completion on this str attribute will list all the vectorized string methods\n",
    "# available to Pandas. \n",
    "monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam','Eric Idle', \n",
    "                   'Terry Jones', 'Michael Palin'])\n",
    "monte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Methods similar to Python string methods\n",
    "Nearly all Python’s built-in string methods are mirrored by a Pandas vectorized string\n",
    "method. Here is a list of Pandas str methods that mirror Python string methods:\n",
    "len() lower() translate() islower()\n",
    "ljust() upper() startswith() isupper()\n",
    "rjust() find() endswith() isnumeric()\n",
    "center() rfind() isalnum() isdecimal()\n",
    "zfill() index() isalpha() split()\n",
    "strip() rindex() isdigit() rsplit()\n",
    "rstrip() capitalize() isspace() partition()\n",
    "lstrip() swapcase() istitle() rpartition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that these have various return values. \n",
    "# some return numbers:\n",
    "monte.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some others, like lower(), return a series of string\n",
    "monte.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or Boolean values:\n",
    "monte.str.startswith('T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still others return lists or other compound values for each element:\n",
    "monte.str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods using regular expressions\n",
    "In addition, there are several methods that accept regular expressions to examine the content of\n",
    "each string element, and follow some of the API conventions of Python’s built-in re module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#match() Call re.match() on each element, returning a Boolean.\n",
    "#extract() Call re.match() on each element, returning matched groups as strings.\n",
    "#findall() Call re.findall() on each element.\n",
    "#replace() Replace occurrences of pattern with some other string.\n",
    "#contains() Call re.search() on each element, returning a Boolean.\n",
    "#count() Count occurrences of pattern.\n",
    "#split() Equivalent to str.split(), but accepts regexps.\n",
    "#rsplit() Equivalent to str.rsplit(), but accepts regexps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, we can extract the first name from each by asking for a contiguous group of \n",
    "# characters at the beginning of each element:\n",
    "monte.str.extract('([A-Za-z]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can do something more complicated, like finding all names that start and end with a \n",
    "# consonant, making use of the start-of-string (^) and end-of-string ($) regular expression:\n",
    "monte.str.findall(r'^[^AEIOU].*[^aeiou]$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous methods\n",
    "#Finally, there are some miscellaneous methods that enable other convenient operations\n",
    "#Method Description\n",
    "#get() Index each element\n",
    "#slice() Slice each element\n",
    "#slice_replace() Replace slice in each element with passed value\n",
    "#cat() Concatenate strings\n",
    "#repeat() Repeat values\n",
    "#normalize() Return Unicode form of string\n",
    "#pad() Add whitespace to left, right, or both sides of strings\n",
    "#wrap() Split long strings into lines with length less than a given width\n",
    "#join() Join strings in each element of the Series with passed separator\n",
    "#get_dummies() Extract dummy variables as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get a slice of the first three characters of each array using str.slice(0, 3).\n",
    "print(monte.str.slice(0, 3), '\\n')\n",
    "print(monte.str[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, to extract the last name of each entry, we can combine split() and get():\n",
    "monte.str.split().str.get(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indicator variables. Another method that requires a bit of extra explanation is the\n",
    "get_dummies() method. This is useful when your data has a column containing some\n",
    "sort of coded indicator. For example, we might have a dataset that contains informa‐\n",
    "tion in the form of codes, such as A=“born in America,” B=“born in the United King‐\n",
    "dom,” C=“likes cheese,” D=“likes spam”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "full_monte = pd.DataFrame({'name': monte, 'info': ['B|C|D', 'B|D', 'A|C', 'B|D', 'B|C', 'B|C|D']})\n",
    "full_monte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_dummies() routine lets you quickly split out these indicator variables into a DataFrame:\n",
    "full_monte['info'].str.get_dummies('|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Using Recipe Database\n",
    "These vectorized string operations become most useful in the process of cleaning up\n",
    "messy, real-world data. Here I’ll walk through an example of that, using an open\n",
    "recipe database compiled from various sources on the Web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    recipes = pd.read_json('C:\\\\Users\\\\OWNER\\\\Datasets\\\\recipeitems-latest\\\\recipeitems-latest.json')\n",
    "except ValueError as e:\n",
    "    print(\"ValueError:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get a ValueError mentioning that there is “trailing data.”it seems that it’s due to using a \n",
    "# file in which each line is itself a valid JSON, but the full file is not. Let’s check if this\n",
    "# interpretation is true:\n",
    "with open('C:\\\\Users\\\\OWNER\\\\Datasets\\\\recipeitems-latest\\\\recipeitems-latest.json') as f:\n",
    "    line = f.readline()\n",
    "pd.read_json(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes, apparently each line is a valid JSON, so we’ll need to string them together. \n",
    "# read the entire file into a Python array\n",
    "with open('C:\\\\Users\\\\OWNER\\\\Datasets\\\\recipeitems-latest\\\\recipeitems-latest.json', 'r',\n",
    "          encoding='utf-8', errors='ignore') as f:\n",
    "    # Extract each line\n",
    "    data = (line.strip() for line in f)\n",
    "    # Reformat so each line is the element of a list\n",
    "    data_json = \"[{0}]\".format(','.join(data))\n",
    "# read the result as a JSON\n",
    "recipes = pd.read_json(data_json)\n",
    "recipes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let’s take a look at the dataset:\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let’s take a look at one row to see what we have:\n",
    "recipes.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s start by taking a closer look at the ingredients:\n",
    "recipes.ingredients.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just out of curiosity, let’s see which recipe has the longest ingredient list:\n",
    "recipes.name[np.argmax(recipes.ingredients.str.len())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.description[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do other aggregate explorations; for example, let’s see how many of the recipes are\n",
    "# for breakfast food:\n",
    "recipes.description.str.contains('[Bb]reakfast').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or how many of the recipes list cinnamon as an ingredient:\n",
    "recipes.ingredients.str.contains('[Cc]innamon').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could even look to see whether any recipes misspell the ingredient as “cinamon”:\n",
    "recipes.ingredients.str.contains('[Cc]inamon').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a recipe that uses all those ingredients, to extract a clean list of ingredients from \n",
    "# each row.\n",
    "spice_list = ['salt', 'pepper', 'oregano', 'sage', 'parsley',\n",
    "'rosemary', 'tarragon', 'thyme', 'paprika', 'cumin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then build a Boolean DataFrame consisting of True and False values, indicating whether\n",
    "# this ingredient appears in the list:\n",
    "import re\n",
    "spice_df = pd.DataFrame(dict((spice, recipes.ingredients.str.contains(spice, re.IGNORECASE))\n",
    "                             for spice in spice_list))\n",
    "spice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, as an example, let’s say we’d like to find a recipe that uses parsley, paprika, and\n",
    "# tarragon. We can compute this very quickly using the query()\n",
    "selection = spice_df.query('parsley & paprika & tarragon')\n",
    "len(selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s use the index returned by this selection to discover the names of the recipes that \n",
    "# have this combination:\n",
    "recipes.name[selection.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with Time series\n",
    "Date and time data comes in a few flavors, which we will discuss here:\n",
    "• Time stamps reference particular moments in time (e.g., July 4th, 2015, at 7:00\n",
    "a.m.).\n",
    "• Time intervals and periods reference a length of time between a particular beginning and end point for example, the year 2015. \n",
    "• Time deltas or durations reference an exact length of time (e.g., a duration of\n",
    "22.56 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the Native Python dates and times: datetime and dateutil\n",
    "# For example, you can manually build a date using the datetime type:\n",
    "from datetime import datetime\n",
    "date = datetime(year=2015, month=7, day=4)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the dateutil module, you can parse dates from a variety of string formats:\n",
    "from dateutil import parser\n",
    "date = parser.parse(\"4th of July, 2015\")\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you have a datetime object, you can do things like printing the month of the year:\n",
    "date.strftime('%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_now = date.now()\n",
    "date_now.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typed arrays of times: NumPy’s datetime64\n",
    "# The datetime64 dtype encodes dates as 64-bit integers, and thus allows arrays of dates to be\n",
    "# represented very compactly. \n",
    "date = np.array('2015-07-04', dtype=np.datetime64)\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have this date formatted, however, we can quickly do vectorized operations on it:\n",
    "date + np.arange(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a day-based datetime:\n",
    "np.datetime64(\"2020-02-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a minute-based datetime:\n",
    "np.datetime64(\"2020-02-12 02:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can force any desired fundamental unit using one of many format codes; for example, here \n",
    "# we’ll force a nanosecond-based time:\n",
    "np.datetime64(\"2020-02-12 02:40:30.50\", \"ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dates and times in Pandas: Best of both worlds\n",
    "# We can parse a flexibly formatted string date,\n",
    "pd_date = pd.to_datetime(\"4th of Feb, 2020\")\n",
    "pd_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the formated codes to output the day of the week:\n",
    "pd_date.strftime('%A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do NumPy-style vectorized operations directly on this same object:\n",
    "pd_date + pd.to_timedelta(np.arange(12), \"D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Time Series: Indexing by Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can construct a Series object that has timeindexed data:\n",
    "index = pd.DatetimeIndex(['2018-07-04', '2019-07-04', '2018-08-04', '2019-08-04'])\n",
    "date_data = pd.Series([0, 1, 2, 3], index= index)\n",
    "date_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have this data in a Series, we can make use of any of the Series indexing patterns\n",
    "date_data['2018-07-04':'2019-07-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are additional special date-only indexing operations, such as passing a year to\n",
    "# obtain a slice of all data from that year:\n",
    "date_data['2018']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Time Series Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing a single date to pd.to_datetime() yields a Timestamp; passing a series of dates by\n",
    "# default yields a DatetimeIndex:\n",
    "dates = pd.to_datetime([datetime(2015, 7, 3), '4th of July, 2015',\n",
    "'2015-Jul-6', '07-07-2015', '20150708'])\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any DatetimeIndex can be converted to a PeriodIndex with the to_period() function with the \n",
    "# addition of a frequency code; here we’ll use 'D' to indicate daily frequency:\n",
    "dates.to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A TimedeltaIndex is created, for example, when one date is subtracted from another:\n",
    "dates - dates[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular sequences: pd.date_range()\n",
    "To make the creation of regular date sequences more convenient, Pandas offers a few\n",
    "functions for this purpose: pd.date_range() for timestamps, pd.period_range() for\n",
    "periods, and pd.timedelta_range() for time deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, pd.date_range() accepts a start date, an end date, and an optional frequency code to\n",
    "# create a regular sequence of dates. By default, the frequency is one day:\n",
    "pd.date_range('2015-07-03', '2015-07-10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the date range can be specified not with a start- and endpoint, but with a startpoint and a \n",
    "# number of periods:\n",
    "pd.date_range('2015-07-03', periods=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can modify the spacing by altering the freq argument, which defaults to D. For example, here\n",
    "# we will construct a range of hourly timestamps:\n",
    "pd.date_range('2015-07-03', periods=8, freq='H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create regular sequences of period or time delta values, the very similar pd.period_range() \n",
    "# and pd.timedelta_range() functions are useful. Here are some monthly periods:\n",
    "pd.period_range('2015-07-3', periods=8, freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And a sequence of durations increasing by an hour:\n",
    "pd.timedelta_range(0, periods=10, freq='H')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequencies and Oﬀsets\n",
    "#D= Calendar day B= Business day\n",
    "#W= Weekly       M= Month end \n",
    "#Q= Quarter end  BQ= Business quarter end\n",
    "#A= Year end     BA= Business year end\n",
    "#H= Hours        BH= Business hours\n",
    "#T= Minutes      S= Seconds\n",
    "#L= Milliseonds  U= Microseconds\n",
    "#N= Nanoseconds  BM= Business month end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a frequency of 2 hours 30 minutes, we can combine the hour (H) and minute (T) codes as:\n",
    "pd.timedelta_range(0, periods=9, freq=\"2H30T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, we can create a business day offset directly as follows: \n",
    "from pandas.tseries.offsets import BDay\n",
    "pd.date_range('2015-07-01', periods=5, freq=BDay())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling, Shifting, and Windowing\n",
    "The ability to use dates and times as indices to intuitively organize and access data is\n",
    "an important piece of the Pandas time series tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using some stock price data as an example. Here we will load google’s closing price history\n",
    "# You need to install pandas_datareader using pip\n",
    "from pandas_datareader import data\n",
    "goog = data.DataReader('GOOG', start='2004', end='2016', data_source='yahoo')\n",
    "goog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we’ll use just the closing price:\n",
    "goog = goog['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize this using the plot() method, after the normal Matplotlib setup\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "goog.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling and converting frequencies\n",
    "One common need for time series data is resampling at a higher or lower frequency.\n",
    "You can do this using the resample() method, or the much simpler asfreq() method. The primary \n",
    "difference between the two is that resample() is fundamentally a data aggregation, while asfreq() \n",
    "is fundamentally a data selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will resample the data at the end of business year\n",
    "goog.plot(alpha=0.5, style='-')\n",
    "goog.resample('BA').mean().plot(style=':')\n",
    "goog.asfreq('BA').plot(style='--');\n",
    "plt.legend(['input', 'resample', 'asfreq'], loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will resample the business day data at a daily frequency (i.e., including weekends)\n",
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "data = goog.iloc[:10]\n",
    "data.asfreq('D').plot(ax=ax[0], marker='o')\n",
    "data.asfreq('D', method='bfill').plot(ax=ax[1], style='-o')\n",
    "data.asfreq('D', method='ffill').plot(ax=ax[1], style='--o')\n",
    "ax[1].legend([\"back-fill\", \"forward-fill\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-shifts\n",
    "Another common time series–specific operation is shifting of data in time. Pandas\n",
    "has two closely related methods for computing this: shift() and tshift(). In short,\n",
    "the difference between them is that shift() shifs the data, while tshift() shifs the\n",
    "index. In both cases, the shift is specified in multiples of the frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will both shift() and tshift() by 900 days:\n",
    "fig, ax = plt.subplots(3, sharey=True)\n",
    "# apply a frequency to the data\n",
    "goog = goog.asfreq('D', method='pad')\n",
    "goog.plot(ax=ax[0])\n",
    "goog.shift(900).plot(ax=ax[1])\n",
    "goog.tshift(900).plot(ax=ax[2])\n",
    "# legends and annotations\n",
    "local_max = pd.to_datetime('2007-11-05')\n",
    "offset = pd.Timedelta(900, 'D')\n",
    "ax[0].legend(['input'], loc=2)\n",
    "ax[0].get_xticklabels()[4].set(weight='heavy', color='red')\n",
    "ax[0].axvline(local_max, alpha=0.3, color='red')\n",
    "ax[1].legend(['shift(900)'], loc=2)\n",
    "ax[1].get_xticklabels()[4].set(weight='heavy', color='red')\n",
    "ax[1].axvline(local_max + offset, alpha=0.3, color='red')\n",
    "ax[2].legend(['tshift(900)'], loc=2)\n",
    "ax[2].get_xticklabels()[1].set(weight='heavy', color='red')\n",
    "ax[2].axvline(local_max + offset, alpha=0.3, color='red');\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use shifted values to compute the one-year return on investment for\n",
    "# google stock over the course of the dataset \n",
    "ROI = 100 * (goog.tshift(-365) / goog - 1)\n",
    "ROI.plot()\n",
    "plt.ylabel('% Return on Investment');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rolling windows\n",
    "Rolling statistics are a third type of time series–specific operation implemented by\n",
    "Pandas. These can be accomplished via the rolling() attribute of Series and Data\n",
    "Frame objects, which returns a view similar to what we saw with the groupby operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, here is the one-year centered rolling mean and standard deviation of the\n",
    "# google stock prices:\n",
    "rolling = goog.rolling(365, center=True)\n",
    "data = pd.DataFrame({'input': goog, 'one-year rolling_mean': rolling.mean(),\n",
    "                     'one-year rolling_std': rolling.std()})\n",
    "ax = data.plot(style=['-', '--', ':'])\n",
    "ax.lines[0].set_alpha(0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Visualizing Seattle Bicycle Counts\n",
    "let’s take a look at bicycle counts on Seattle’s Fremont Bridge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use Pandas to read the CSV output into a DataFrame. We will specify that we want the Date\n",
    "# as an index, and we want these dates to be automatically parsed:\n",
    "data = pd.read_csv('C:\\\\PythonClass\\\\Data Science\\\\Datasets\\\\fremont-bridge.csv', index_col='Date',\n",
    "                   parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convenience, we’ll further process this dataset by shortening the column names and adding\n",
    "# a “Total” column:\n",
    "data.columns = ['West', 'East']\n",
    "data['Total'] = data.eval('West + East')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let’s take a look at the summary statistics for this data:\n",
    "data.dropna().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can gain some insight into the dataset by visualizing it. Let’s start by plotting the\n",
    "# raw data:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn; seaborn.set()\n",
    "data.plot()\n",
    "plt.ylabel('Hourly Bicycle Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ~25,000 hourly samples are far too dense for us to make much sense of. We can\n",
    "# gain more insight by resampling the data to a coarser grid. Let’s resample by week\n",
    "weekly = data.resample('W').sum()\n",
    "weekly.plot(style=[':', '--', '-'])\n",
    "plt.ylabel('Weekly bicycle count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way that comes in handy for aggregating the data is to use a rolling mean, utilizing the\n",
    "# pd.rolling_mean() function. Here we’ll do a 30-day rolling mean of our data, making sure to \n",
    "# center the window\n",
    "daily = data.resample('D').sum()\n",
    "daily.rolling(30, center=True).sum().plot(style=[':', '--', '-'])\n",
    "plt.ylabel('mean hourly count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code specifies both the width of the window (we chose 50 days) and the width of\n",
    "# the Gaussian within the window (we chose 10 days):\n",
    "daily.rolling(50, center=True, win_type='gaussian').sum(std=10).plot(style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we might want to look at the average traffic as a function of the time of day. We can do this\n",
    "# using the GroupBy functionality\n",
    "by_time = data.groupby(data.index.time).mean()\n",
    "hourly_ticks = 4 * 60 * 60 * np.arange(6)\n",
    "by_time.plot(xticks=hourly_ticks, style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also might be curious about how things change based on the day of the week.\n",
    "# Again, we can do this with a simple groupby\n",
    "by_weekday = data.groupby(data.index.dayofweek).mean()\n",
    "by_weekday.index = ['Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat', 'Sun']\n",
    "by_weekday.plot(style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this in mind, let’s do a compound groupby and look at the hourly trend on weekdays versus \n",
    "# weekends. We’ll start by grouping by both a flag marking the weekend, and the time of day:\n",
    "weekend = np.where(data.index.weekday < 5, 'Weekday', 'Weekend')\n",
    "by_time = data.groupby([weekend, data.index.time]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we’ll use some of the Matplotlib tools  to plot two panels side by side:\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "by_time.loc['Weekday'].plot(ax=ax[0], title='Weekdays',xticks=hourly_ticks, style=[':', '--', '-'])\n",
    "by_time.loc['Weekend'].plot(ax=ax[1], title='Weekends',xticks=hourly_ticks, style=[':', '--', '-']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with query and eva for expressions\n",
    "Pandas includes some experimental tools that allow you to directly access C-speed operations without costly allocation of inter-mediate arrays. These are the eval() and query() function, which rely on the\n",
    "Numexpr package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ve seen previously that NumPy and Pandas support fast vectorized operations; for example, \n",
    "# when you are adding the elements of two arrays:\n",
    "rng = np.random.RandomState(42)\n",
    "x = rng.rand(6)\n",
    "y = rng.rand(6)\n",
    "%timeit x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is much faster than doing the addition via a Python loop or comprehension:\n",
    "%timeit np.fromiter((xi + yi for xi, yi in zip(x, y)), dtype=x.dtype, count=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But this abstraction can become less efficient when you are computing compound expressions. For \n",
    "# example, consider the following expression:\n",
    "mask = (x > 0.5) & (y < 0.5)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because NumPy evaluates each subexpression, this is roughly equivalent to the following:\n",
    "tmp1 = (x > 0.5)\n",
    "tmp2 = (y < 0.5)\n",
    "mask = tmp1 & tmp2\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Numexpr library gives you the ability to compute this type of compound\n",
    "# expression element by element, without the need to allocate full intermediate arrays.\n",
    "import numexpr\n",
    "mask_numexpr = numexpr.evaluate('(x > 0.5) & (y < 0.5)')\n",
    "# check if the two methods returns the same output\n",
    "np.allclose(mask, mask_numexpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas eval() and query() tools that we will discuss here are conceptually similar, and \n",
    "depend on the Numexpr package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eval() function in Pandas uses string expressions to efficiently compute operations using \n",
    "# DataFrames.\n",
    "nrows, ncols = 100000, 100       \n",
    "rng = np.random.RandomState(42)       \n",
    "df1, df2, df3, df4 = (pd.DataFrame(rng.rand(nrows, ncols))                             \n",
    "for i in range(4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the sum of all four DataFrames using the typical Pandas approach, we can just \n",
    "# write the sum:\n",
    "%timeit df1 + df2 + df3 + df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compute the same result via pd.eval by constructing the expression as a string:\n",
    "%timeit pd.eval('df1 + df2 + df3 + df4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eval() version of this expression is about 50% faster (and uses much less memory), while \n",
    "# giving the same result:\n",
    "np.allclose(df1 + df2 + df3 + df4, pd.eval('df1 + df2 + df3 + df4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations supported by pd.eval()\n",
    "As of Pandas v0.16, pd.eval() supports a wide range of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To demonstrate these, we’ll use the following integer DataFrames:\n",
    "df1, df2, df3, df4, df5 = (pd.DataFrame(rng.randint(0, 1000, (100, 3))) for i in range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.eval() supports all arithmetic operators. For example:\n",
    "result1 = -df1 * df2 / (df3 + df4) - df5\n",
    "result2 = pd.eval('-df1 * df2 / (df3 + df4) - df5')\n",
    "np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.eval() supports all comparison operators, including chained expressions:\n",
    "result1 = (df1 < df2) & (df2 <= df3) & (df3 != df4)\n",
    "result2 = pd.eval('df1 < df2 <= df3 != df4')\n",
    "np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.eval() supports the & and | bitwise operators:\n",
    "result1 = (df1 < 0.5) & (df2 < 0.5) | (df3 < df4)\n",
    "result2 = pd.eval('(df1 < 0.5) & (df2 < 0.5) | (df3 < df4)')\n",
    "np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it supports the use of the literal and and or in Boolean expressions:\n",
    "result3 = pd.eval('(df1 < 0.5) and (df2 < 0.5) or (df3 < df4)')\n",
    "np.allclose(result1, result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.eval() supports access to object attributes via the obj.attr syntax, and indexes via the \n",
    "# obj[index] syntax:\n",
    "result1 = df2.T[0] + df3.iloc[1]\n",
    "result2 = pd.eval('df2.T[0] + df3.iloc[1]')\n",
    "np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame.eval() for Column-Wise Operations\n",
    "Just as Pandas has a top-level pd.eval() function, DataFrames have an eval() method that works in similar ways. The benefit of the eval() method is that columns can be referred to by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We’ll use this labeled array as an example:\n",
    "df = pd.DataFrame(rng.rand(1000, 3), columns=['A', 'B', 'C'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pd.eval() as above, we can compute expressions with the three columns like this:\n",
    "result1 = (df['A'] + df['B']) / (df['C'] - 1)\n",
    "result2 = pd.eval(\"(df.A + df.B) / (df.C - 1)\")\n",
    "np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataFrame.eval() method allows much more succinct evaluation of expressions with the columns\n",
    "result3 = df.eval('(A + B) / (C - 1)')\n",
    "np.allclose(result1, result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use df.eval() to create a new column 'D' and assign to it a value computed from the \n",
    "# other columns:\n",
    "df.eval('D = (A + B) / C', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the same way, any existing column can be modified:\n",
    "df.eval('D = (A - B) / C', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DataFrame.eval() method supports an additional syntax that lets it work with local Python\n",
    "# variables usig @ as reference to the variable. Consider the following:\n",
    "column_mean = df.mean(axis=1)\n",
    "result1 = df['A'] + column_mean\n",
    "result2 = df.eval('A + @column_mean')\n",
    "np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this @ character is only supported by the DataFrame.eval() method, not by the pandas.eval() function, because the pandas.eval() function only has access to the one (Python) namespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame.query() Method\n",
    "The DataFrame has another method based on evaluated strings, called the query() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the following:\n",
    "result1 = df[(df.A < 0.5) & (df.B < 0.5)]\n",
    "result2 = pd.eval('df[(df.A < 0.5) & (df.B < 0.5)]')\n",
    "np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an expression involving columns of the DataFrame. It cannot be expressed using the Data\n",
    "# Frame.eval() syntax, however! Instead, for this type of filtering operation, you can use the \n",
    "# query() method:\n",
    "result2 = df.query('A < 0.5 and B < 0.5')\n",
    "np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the query() method also accepts the @ flag to mark local variables:\n",
    "Cmean = df['C'].mean()\n",
    "result1 = df[(df.A < Cmean) & (df.B < Cmean)]\n",
    "result2 = df.query('A < @Cmean and B < @Cmean')\n",
    "np.allclose(result1, result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of this class on Pandas Library. Thanks for your time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
